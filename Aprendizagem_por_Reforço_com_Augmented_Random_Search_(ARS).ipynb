{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aprendizagem por Reforço com Augmented Random Search (ARS).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM18I9csjOUq+ESJun+1jqk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mantoan-thi/Aprendizagem-por-Refor-o-com-Augmented-Random-Search-ARS-/blob/main/Aprendizagem_por_Refor%C3%A7o_com_Augmented_Random_Search_(ARS).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naDAX7b08u8x"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import wrappers\n",
        "\n",
        "class Hp():\n",
        "  def __init__(self):\n",
        "    self.nb_steps = 1000         # número de loops que vamos utilizar para o treinamento.\n",
        "    self.episode_lenght = 1000   # máxima duração de um epsódio.\n",
        "    self.learning_rate = 0.002   # Taxa de aprendizagem.\n",
        "    self.nb_directions = 16      # número de direções -  quantidade de matrizes...\n",
        "    self.nb_best_directions = 16 # seleciona as melhores matrizes.\n",
        "    assert self. nb_best_directions <= self.nb_directions # nb_best_directions tem que ser menor ou igual ao self.nb_directions.\n",
        "    self.noise = 0.03            # Usado para não termos variância nos dados.\n",
        "    self.seed = 1                # usamos para termos os mesmos resultados.\n",
        "    self.env_name = ''           # nome do ambiente.\n",
        "\n",
        "class Normalizer():\n",
        "  def __init__(self,nb_inputs):\n",
        "    self.n = np.zeros(np_inputs)          # Quantidade de entradas.\n",
        "    self.mean = np.zeros(np_inputs)       # Média\n",
        "    self.mean_diff = np.zeros(np_inputs)  # \n",
        "    self.var = np.zeros(np_inputs)        # Variância\n",
        "  def observe(self,x):\n",
        "    self.n +=1.\n",
        "    last_mean = self.mean.copy()\n",
        "    self.mean += (x-self.mean)/self.n\n",
        "    self.mean_diff += (x -last_mean)*(x - self.mean)\n",
        "    self.var = (self.mean_diff/self.n).clip(min = 1e-2) #0.001\n",
        "  def normalizer(self,inputs):\n",
        "    obs_mean = self.mean\n",
        "    obs_std = np.sqrt(self.var)\n",
        "    return (inputs -obs_mean)/obs_std\n",
        "\n",
        "class Policy():\n",
        "  def __init__(self,input_size,output_size):\n",
        "    self.theta = np.zeros((output_size,input_size)) # serve para atualizar os pesos.\n",
        "\n",
        "  def evaluate (self,input,delta=None,direction = None):\n",
        "    if direction is None:\n",
        "      return self.theta.dot(input)\n",
        "    elif direction == 'positive':\n",
        "      return (self.theta +hp.noise * delta).dot(input)\n",
        "    else:\n",
        "      return (self.theta -hp.noise * delta).dot(input)\n",
        "  def sample_deltas(self):\n",
        "    return [np.random.randn(*self.theta.shape) for _ in range(Hp.nb_directions)]\n",
        "\n",
        "  # Atualização dos pesos\n",
        "  def update(self,rollouts, sigma_r):\n",
        "    step = np.zeros(self.theta.shape)\n",
        "    for r_pos, r_neg, d in rollouts:\n",
        "      step +=(r_pos - r_neg)*d\n",
        "    self.theta +=Hp.learning_rate/(Hp.nb_best_directions*sigma_r)*step\n",
        "\n",
        "# Explora a política em uma direção específica e dentro de um epsódio\n",
        "def explore(env,normalizer,policy,direction = None, delta = None):\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  num_plays = 0.\n",
        "  sum_rewards = 0\n",
        "  while not done and num_plays < Hp.episode_lenght:\n",
        "    normalizer.observe(state)\n",
        "    state = normalizer.normalizer(state)\n",
        "    action = policy.evaluate(state,delta,direction)\n",
        "    state,rewards,done,_ = env.step(action)\n",
        "    reward = max(min(reward,),-1)\n",
        "    sum_rewards +=reward\n",
        "    num_plays +=1\n",
        "  return sum_rewards\n",
        "\n",
        "# Treinamento da inteligência artifical\n",
        "def train(env,policy,normalizer,Hp):\n",
        "  for step in range(Hp.nb_steps):\n",
        "    # Inicialização das pertubações (delta) e as recompensas positivas e negativas\n",
        "    delta = policy.sample_deltas()\n",
        "    positive_rewards = [0]*Hp.np_directions\n",
        "    negative_rewards = [0]*Hp.np_directions\n",
        "    # Obtenção da recompensas das direções positivas.\n",
        "    for k in range(Hp.nb_directions):\n",
        "      positive_rewards[k] = explore(env,normalizer,direction='positive',delt=delta[k])\n",
        "    # Obtenção da recompensas das direções negativas.\n",
        "    for k in range(Hp.nb_directions):\n",
        "      negative_rewards[k] = explore(env,normalizer,direction='negative',delt=delta[k])\n",
        "    # Obtendo todas as recompensas positivas e negativas para computar o desvio padrão dessas recompensas\n",
        "    all_rewards = np.array(positive_rewards + negative_rewards)\n",
        "    sigma_r = all_rewards.std()\n",
        "    # Ordenação dos dollouts e seleção das melhrores direções\n",
        "    scores ={k:max(r_pos,r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards,negative_rewards))}\n",
        "    order = sorted(scores.keys(),key=lambda x: scores[x],reverse=True)[:Hp.nb_best_directions]\n",
        "    rollouts = [(positive_rewards[k],negative_rewards[k],delta[k]) for k in order]\n",
        "    # Atualização da política\n",
        "    policy.update(rollouts,sigma_r)\n",
        "    # impressão da recompensa final depois da atualização\n",
        "    reward_evaluation = explore(env,normalizer,policy)\n",
        "    print('Step',step,'Reward', reward_evaluation)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNGx0N7IPLjc"
      },
      "source": [
        "def mkdir(base,name):\n",
        "  path = os.path.join(base,name)\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "  return path\n",
        "work_dir = mkdir('exp','brs')\n",
        "monitor_dir = mkdir(work_dir,'monitor')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "3mbIG8BLPVIv",
        "outputId": "6be70a29-7a30-424d-be71-ac7b07b682c4"
      },
      "source": [
        "hp = Hp()\n",
        "np.random.seed(hp.seed)\n",
        "env = gym.make(hp.env_name)\n",
        "env = wrapper.Monitor(env,monitor_dir,force = True)\n",
        "nb_inputs = env.observation_space.shape[0]\n",
        "nb_outputs = env.action_space.shape[0]\n",
        "policy = Policy.(nb_inputs,nb_outputs)\n",
        "normalizer = Normalizer(nb_inputs)\n",
        "train(env,policy,normalizer,hp)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-02049f444a0c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    hp. = Hp()\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}